{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to predict graph embedding from paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TQDM dark mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T21:50:23.449469500Z",
     "start_time": "2024-02-04T21:49:54.255294900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "with open(Path('F:\\\\') / 'data' / 'prov_dp' / 'tc3-trace-training.pkl', 'rb') as f:\n",
    "    data_unflattened = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T21:51:09.091575Z",
     "start_time": "2024-02-04T21:51:09.051864500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "dataset = list(chain.from_iterable(data_unflattened))\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embedding_size = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 tokens in 1489123 entries\n",
      "Distinct paths: 37\n"
     ]
    }
   ],
   "source": [
    "tokens = set()\n",
    "distinct_paths = set()\n",
    "\n",
    "def tokenize(path):\n",
    "    assert len(path) % 2 == 0\n",
    "    return [f'{path[idx]}|{path[idx+1]}' for idx in range(0, len(path), 2)]\n",
    "\n",
    "for path, _ in dataset:\n",
    "    path = tokenize(path.split(' '))\n",
    "    tokens.update(path)\n",
    "    distinct_paths.add(' '.join(path))\n",
    "tokens = list(tokens)\n",
    "tokens[:0] = ['.']\n",
    "itos = tokens\n",
    "stoi = {\n",
    "    token: i for i, token in enumerate(itos)\n",
    "}\n",
    "\n",
    "print(f'Found {len(tokens)} tokens in {len(dataset)} entries')\n",
    "print(f'Distinct paths: {len(distinct_paths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\workspace\\prov-dp\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to NetworkX graphs: 100%|██████████| 1489123/1489123 [00:23<00:00, 63455.25it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1489123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from source.algorithm.utility import to_nx\n",
    "\n",
    "nx_graphs = [\n",
    "    to_nx(graph)\n",
    "    for _, graph in tqdm(dataset, desc='Converting to NetworkX graphs')\n",
    "]\n",
    "len(nx_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1, Max: 8\n",
      "Avg: 1.0003753887355176, Std: 0.03720074179980315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lens = [len(g) for p, g in dataset]\n",
    "print(f'Min: {min(lens)}, Max: {max(lens)}')\n",
    "print(f'Avg: {np.average(lens)}, Std: {np.std(lens)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train graph2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import Graph2Vec\n",
    "\n",
    "graph2vec = Graph2Vec(\n",
    "    wl_iterations=80,\n",
    "    attributed=True,\n",
    "    dimensions=embedding_size,\n",
    "    workers=4,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "graph2vec.fit(nx_graphs)\n",
    "with open(Path('F:\\\\') / 'data' / 'prov_dp' / 'tc3-trace-graph2vec.pkl', 'wb') as file:\n",
    "    pickle.dump(graph2vec, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import Graph2Vec\n",
    "with open(Path('F:\\\\') / 'data' / 'prov_dp' / 'tc3-trace-graph2vec.pkl', 'rb') as file:\n",
    "    graph2vec = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489123"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_embeddings = graph2vec.get_embedding()\n",
    "normalized_embeddings = [ \n",
    "    v/np.linalg.norm(v) \n",
    "    for v in graph_embeddings\n",
    "    ]\n",
    "len(graph_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1489123, 8]) torch.Size([1489123, 64])\n"
     ]
    }
   ],
   "source": [
    "## Format training data\n",
    "X, Y = [], []\n",
    "\n",
    "for i, row in enumerate(dataset):\n",
    "    path_str, _ = row\n",
    "    path_list = tokenize(path_str.split(' '))\n",
    "    path = [stoi[s] for s in path_list]\n",
    "    context = [0] * block_size\n",
    "    for i in range(min(len(path), block_size)):\n",
    "        context[-i-1] = path[i]\n",
    "    X.append(context)    \n",
    "    Y.append(normalized_embeddings[i])\n",
    "\n",
    "X = torch.tensor(X, device=device)\n",
    "Y = torch.tensor(Y, device=device)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linalg.norm(normalized_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 8, 8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "n_embedding = 10 # Embedding dimension\n",
    "n_hidden = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 32768\n",
    "block_size = 8\n",
    "\n",
    "# Predict embedding, then lookup closest embedding from graph2vec... gradients don't flow through to graph2vec though :(\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, n_embedding),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(n_embedding * block_size, n_embedding * block_size), nn.LeakyReLU(),\n",
    "    nn.Linear(n_embedding * block_size, n_hidden), nn.LeakyReLU(),\n",
    "    nn.Linear(n_hidden, embedding_size)\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "torch.Size([8, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x10 and 80x80)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m layers[\u001b[38;5;241m1\u001b[39m](x)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Jonathan\\workspace\\prov-dp\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jonathan\\workspace\\prov-dp\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jonathan\\workspace\\prov-dp\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x10 and 80x80)"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Embedding(vocab_size, n_embedding),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(n_embedding * block_size, n_embedding * block_size), nn.ReLU(),\n",
    "    nn.Linear(n_embedding * block_size, n_hidden), nn.ReLU(),\n",
    "    nn.Linear(n_hidden, embedding_size)\n",
    "    ]\n",
    "x = X[0]\n",
    "x = layers[0](x)\n",
    "print(x.shape)\n",
    "x = layers[1](x)\n",
    "print(x.shape)\n",
    "\n",
    "x = layers[2](x)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/   1000: 0.0343036\n",
      "    100/   1000: 0.0042134\n",
      "    200/   1000: 0.0022222\n",
      "    300/   1000: 0.0018093\n",
      "    400/   1000: 0.0016219\n",
      "    500/   1000: 0.0014289\n",
      "    600/   1000: 0.0013052\n",
      "    700/   1000: 0.0011813\n",
      "    800/   1000: 0.0010690\n",
      "    900/   1000: 0.0009849\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "max_steps = 1000\n",
    "for i in range(max_steps):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ix = torch.randint(0, Y.shape[0], (batch_size,))\n",
    "    X_batch, Y_batch = X[ix], Y[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(X_batch) # output layer\n",
    "    # print(embedding)\n",
    "    # print(Y_batch)\n",
    "    loss = F.mse_loss(output, Y_batch)\n",
    "    # print(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    # clip_grad_norm(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track stats\n",
    "    if i % 100 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.7f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 512028 KiB | 659401 KiB |   1105 GiB |   1105 GiB |\n",
      "|       from large pool | 509070 KiB | 656526 KiB |    783 GiB |    782 GiB |\n",
      "|       from small pool |   2957 KiB |   7058 KiB |    322 GiB |    322 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 512028 KiB | 659401 KiB |   1105 GiB |   1105 GiB |\n",
      "|       from large pool | 509070 KiB | 656526 KiB |    783 GiB |    782 GiB |\n",
      "|       from small pool |   2957 KiB |   7058 KiB |    322 GiB |    322 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 511562 KiB | 658936 KiB |   1100 GiB |   1100 GiB |\n",
      "|       from large pool | 508614 KiB | 656070 KiB |    778 GiB |    777 GiB |\n",
      "|       from small pool |   2947 KiB |   7051 KiB |    322 GiB |    322 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 714752 KiB | 714752 KiB | 714752 KiB |      0 B   |\n",
      "|       from large pool | 706560 KiB | 706560 KiB | 706560 KiB |      0 B   |\n",
      "|       from small pool |   8192 KiB |   8192 KiB |   8192 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  14308 KiB |  59448 KiB |   1123 GiB |   1123 GiB |\n",
      "|       from large pool |   9073 KiB |  54129 KiB |    765 GiB |    765 GiB |\n",
      "|       from small pool |   5234 KiB |   5402 KiB |    357 GiB |    357 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      64    |      75    |    2424 K  |    2424 K  |\n",
      "|       from large pool |       8    |      23    |     208 K  |     208 K  |\n",
      "|       from small pool |      56    |      62    |    2216 K  |    2216 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      64    |      75    |    2424 K  |    2424 K  |\n",
      "|       from large pool |       8    |      23    |     208 K  |     208 K  |\n",
      "|       from small pool |      56    |      62    |    2216 K  |    2216 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      21    |      21    |      21    |       0    |\n",
      "|       from large pool |      17    |      17    |      17    |       0    |\n",
      "|       from small pool |       4    |       4    |       4    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      12    |      21    |     812 K  |     812 K  |\n",
      "|       from large pool |       4    |      12    |     147 K  |     147 K  |\n",
      "|       from small pool |       8    |      11    |     665 K  |     665 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
